# Traditional Evaluation Metrics - Documentation

## Overview

This document explains the **traditional evaluation framework** for the grocery shopping recommendation systems. Unlike LLM-based evaluation (which uses GPT-5 for subjective quality assessment), traditional metrics provide **objective, quantitative measurements** using proven industry-standard formulas.

## Why Traditional Metrics?

### Advantages
- âœ… **No API costs** - Unlike GPT-5 evaluation
- âœ… **Instant results** - No waiting for LLM responses
- âœ… **Objective & repeatable** - Same input = same output
- âœ… **Industry standard** - Used by Amazon, Netflix, Spotify
- âœ… **Easy to track** - Monitor improvements over time
- âœ… **Scientific rigor** - Based on peer-reviewed research

### Use Cases
- **A/B Testing**: Compare different recommendation algorithms
- **Performance Monitoring**: Track system quality over time
- **Optimization**: Identify which metrics need improvement
- **Reporting**: Provide data-driven insights to stakeholders

---

## Evaluation Results (Live Session)

### Latest Results with Real Purchase History

| System | Recommendations | Total Savings | Avg Savings/Item | Savings % | Diversity | Category Match | Reasonable Pricing |
|--------|----------------|---------------|------------------|-----------|-----------|----------------|--------------------|
| **Personalized CF** | **9** | **$50.60** | **$5.62** | **52.7%** | 0.33 | 0.56 | 22.2% |
| **Hybrid AI** | **8** | **$46.60** | **$5.83** | **48.6%** | 0.38 | 0.62 | 25.0% |
| Budget-Saving | 0 | $0.00 | $0.00 | 0.0% | 0.00 | 0.00 | 0.0% |

**Test Scenario:**
- Budget: $80.00
- Cart Total: $82.97
- Over Budget: $2.97
- Cart Items: 5 (Batteries, Laundry Detergent, Beef Jerky, Chocolate, Olive Oil)

---

## Metrics Explained

### 1. Recommendations Count
**Definition**: Number of cheaper alternative products suggested

**Interpretation:**
- More recommendations = more options for users
- Too many (>10) can overwhelm users
- **Optimal range**: 5-8 recommendations

**Current Performance:**
- Personalized CF: 9 âœ… (Good)
- Hybrid AI: 8 âœ… (Optimal)

---

### 2. Total Savings ($)
**Definition**: Sum of all potential savings if all recommendations are accepted

**Formula**: `Î£ (Original Price - Suggested Price) for all recommendations`

**Interpretation:**
- Higher savings = better budget impact
- Should be substantial enough to justify switching
- **Good target**: >40% of cart total

**Current Performance:**
- Personalized CF: $50.60 âœ… (61% of cart total - Excellent!)
- Hybrid AI: $46.60 âœ… (56% of cart total - Excellent!)

---

### 3. Average Savings per Item ($)
**Definition**: Mean saving per recommendation

**Formula**: `Total Savings / Number of Recommendations`

**Interpretation:**
- Shows typical savings magnitude per suggestion
- Higher = more impactful individual recommendations
- **Good target**: >$5/item for grocery shopping

**Current Performance:**
- Personalized CF: $5.62 âœ… (Good)
- Hybrid AI: $5.83 âœ… (Better)

---

### 4. Savings Percentage (%)
**Definition**: Percentage of total cart cost that could be saved

**Formula**: `(Total Savings / Cart Total) Ã— 100`

**Interpretation:**
- Measures overall budget impact
- Higher = more effective at reducing costs
- **Good target**: >40%

**Current Performance:**
- Personalized CF: 52.7% âœ… (Excellent - over half cart cost!)
- Hybrid AI: 48.6% âœ… (Excellent)

---

### 5. Diversity Score
**Definition**: Variety of product categories in recommendations

**Formula**: `Unique Categories / Total Recommendations`

**Range**: 0.0 (all same category) to 1.0 (all different categories)

**Interpretation:**
- Higher = better variety, avoids repetitive suggestions
- Too low (<0.3) = too narrow
- **Good target**: 0.4-0.7

**Current Performance:**
- Personalized CF: 0.33 âš ï¸ (Slightly low - could diversify more)
- Hybrid AI: 0.38 âœ… (Good)

---

### 6. Category Match
**Definition**: How often recommendations match original item's category

**Formula**: `(Matching Categories / Total Recommendations)`

**Range**: 0.0 (no matches) to 1.0 (perfect match)

**Interpretation:**
- Higher = better relevance to user's original choice
- Too low = suggestions may seem random
- **Good target**: 0.5-0.8

**Current Performance:**
- Personalized CF: 0.56 âœ… (Good - 56% match original categories)
- Hybrid AI: 0.62 âœ… (Better - 62% relevance)

---

### 7. Reasonable Pricing (%)
**Definition**: Percentage of recommendations with appropriate discount levels

**Criteria:**
- Discount too small (<5%): Not worth switching
- Discount too large (>70%): Suspiciously cheap, quality concerns
- **Reasonable range**: 5-70% discount

**Formula**: `(Reasonable Recommendations / Total Recommendations) Ã— 100`

**Interpretation:**
- Higher = more realistic, trustworthy suggestions
- Low score = either trivial or unrealistic discounts
- **Good target**: >60%

**Current Performance:**
- Personalized CF: 22.2% âš ï¸ (Low - some extreme discounts)
- Hybrid AI: 25.0% âš ï¸ (Low - needs calibration)

---

## Performance Summary

### ðŸ† Winner: **Personalized CF**
Leads with highest total savings ($50.60) and most recommendations (9).

### ðŸ¥ˆ Runner-up: **Hybrid AI**
Better per-item savings ($5.83), better category match (0.62), and more reasonable pricing (25%).

### Key Insights

#### Strengths
1. **Both systems save 50%+ of cart cost** - Excellent budget impact
2. **Good category matching** - 56-62% relevance to original items
3. **Meaningful savings per item** - $5-6 average savings

#### Areas for Improvement
1. **Reasonable Pricing**: Only 22-25% of recommendations have appropriate discount levels
   - Issue: Some suggestions have extreme discounts (>70%) that may seem unrealistic
   - Fix: Add pricing calibration to filter out suspicious deals

2. **Diversity (CF only)**: Personalized CF at 0.33 could be more diverse
   - Fix: Add diversity penalty in recommendation algorithm

3. **Budget-Saving System**: Not returning recommendations in live session
   - Needs investigation - may be filtering too aggressively

---

## How to Run Evaluation

### Option 1: Evaluate Captured Recommendations (Fastest)
```bash
# Uses recommendations already captured from browser console
python evaluate_captured_recommendations.py
```

**Input**: `captured_recommendations.json`  
**Output**: `live_session_traditional_results.csv`

---

### Option 2: Evaluate with Current Cart (Manual)
```python
from traditional_evaluation_metrics import compare_recommendation_systems

# Define your cart and recommendations
cart = [...]  # Your cart items
budget_saving = [...]  # Budget-saving recommendations
personalized_cf = [...]  # CF recommendations
hybrid_ai = [...]  # Hybrid recommendations

# Run comparison
results_df = compare_recommendation_systems(
    budget_saving,
    personalized_cf,
    hybrid_ai,
    cart
)

# Display results
print(results_df)

# Save to CSV
results_df.to_csv('results.csv', index=False)
```

---

### Option 3: Build History and Evaluate (Full Pipeline)
```bash
# Builds purchase history via API, then evaluates
python build_history_and_evaluate.py
```

This script:
1. Creates 3 purchases through the web app
2. Triggers all 3 recommendation systems
3. Runs traditional evaluation
4. Optionally runs LLM evaluation (GPT-5)

---

## Files in This Framework

### Core Files
- **`traditional_evaluation_metrics.py`** - Metrics library with all formulas
- **`evaluate_systems_traditional.py`** - Standalone runner script
- **`evaluate_captured_recommendations.py`** - Evaluates browser-captured data

### Results Files
- **`live_session_traditional_results.csv`** - Latest evaluation results
- **`captured_recommendations.json`** - Raw recommendation data from live session

### Documentation
- **`TRADITIONAL_METRICS_README.md`** (this file) - Complete documentation

---

## Comparison with LLM Evaluation

| Aspect | Traditional Metrics | LLM Evaluation (GPT-5) |
|--------|-------------------|----------------------|
| **Speed** | Instant | 30-60 seconds |
| **Cost** | Free | ~$0.10 per evaluation |
| **Objectivity** | 100% objective | Subjective judgment |
| **Repeatability** | Perfect | May vary slightly |
| **Use Case** | A/B testing, monitoring | UX quality, user perception |
| **Best For** | Ongoing optimization | Final quality check |

**Recommendation**: Use **both** methods:
- Traditional metrics for rapid iteration and A/B testing
- LLM evaluation for final quality assessment before launch

---

## Next Steps

### 1. Improve Reasonable Pricing Score
Current: 22-25% â†’ Target: 60%+

**Action**: Add discount filters in recommendation logic
```python
# Filter out unreasonable discounts
MIN_DISCOUNT = 0.05  # At least 5% off
MAX_DISCOUNT = 0.70  # No more than 70% off

filtered_recs = [
    rec for rec in recommendations
    if MIN_DISCOUNT <= rec['discount'] <= MAX_DISCOUNT
]
```

### 2. Increase CF Diversity
Current: 0.33 â†’ Target: 0.45+

**Action**: Implement diversity penalty
```python
# Penalize recommendations from overrepresented categories
category_counts = defaultdict(int)
diverse_recs = []

for rec in sorted_recommendations:
    if category_counts[rec['category']] < 2:
        diverse_recs.append(rec)
        category_counts[rec['category']] += 1
```

### 3. Debug Budget-Saving System
**Action**: Investigate why it returns 0 recommendations in live session

---

## References

### Academic Papers
1. Herlocker et al. (2004) - "Evaluating Collaborative Filtering Recommender Systems"
2. Shani & Gunawardana (2011) - "Evaluating Recommendation Systems"
3. McNee et al. (2006) - "Being Accurate is Not Enough"

### Industry Standards
- **Amazon**: Uses similar diversity and category relevance metrics
- **Netflix**: Pioneered many recommendation evaluation techniques
- **Spotify**: Uses both traditional metrics and user engagement data

---

## Questions?

For more details on specific metrics or evaluation methodology, see:
- Source code: `traditional_evaluation_metrics.py`
- Example usage: `evaluate_systems_traditional.py`
- Live session results: `live_session_traditional_results.csv`

---

**Last Updated**: October 21, 2025  
**Version**: 1.0  
**Evaluation Framework**: Traditional Metrics (Objective)
